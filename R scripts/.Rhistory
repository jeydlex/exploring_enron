E(simple.email.graph)$weight
g <- simplify(simple.email.graph, edge.attr.comb=edge.attr.comb=list(weight=function(x) length(x)/sum(1/x),
name="ignore"))
name="ignore"))
g <- simplify(simple.email.graph, edge.attr.comb=list(weight=function(x) length(x)/sum(1/x),
name="ignore"))
E(simple.email.graph)$weight
data <- read.csv(file="data/top_users.csv",head=TRUE,sep=",")
people <- select(data, From, To, Cod, weight)
people <- select(data, From, To, Cod)
email.graph <- graph.data.frame(people, directed=TRUE)
adj <- get.adjacency(email.graph,sparse=FALSE) #to check the content of the matrix
simple.email.graph <- simplify(email.graph, remove.loops = T, remove.multiple = F)
b <- get.adjacency(simple.email.graph,sparse=FALSE) #to see the matrix
View(b)
?get.edgelist()
a <- as_edgelist(simple.email.graph)
View(a)
?get.adjlist()
g <- make_ring(10)
as_adj_list(g)
as_adj_edge_list(g)
d <- as_adj_edge_list(g)
?simplify
simple.email.graph <- simplify(email.graph, remove.loops = T, remove.multiple = F)
b <- get.adjacency(simple.email.graph,sparse=FALSE) #to see the matrix
people <- select(h, From, To, Cod, weight)
people <- select(data, From, To, Cod)
library(dplyr)
library(reshape2)
library(igraph)
data <- read.csv(file="data/top_users.csv",head=TRUE,sep=",")
people <- select(data, From, To, Cod)
email.graph <- graph.data.frame(people, directed=TRUE)
adj <- get.adjacency(email.graph,sparse=FALSE) #to check the content of the matrix
simple.email.graph <- simplify(email.graph, remove.loops = T, remove.multiple = F)
b <- get.adjacency(simple.email.graph,sparse=FALSE) #to see the matrix
write.table(b, file="data/graph_matrix.csv", row.names=FALSE, sep=",")
people <- select(data, From, To)
View(people)
write.table(people, file="data/graph_matrix.csv", row.names=FALSE, sep=",")
new.people <- people
colnames(new.people) <- c('source','target')
View(new.people)
people <- select(data, From, To, Cod)
View(people)
colnames(new.people) <- c('source','target', 'id')
new.people <- people
colnames(new.people) <- c('source','target', 'id')
write.table(people, file="data/graph_matrix.csv", row.names=FALSE, sep=",")
write.table(new.people, file="data/graph_matrix.csv", row.names=FALSE, sep=",")
View(b)
write.table(b, file="data/graph_matrix.csv", row.names=FALSE, sep=",")
?graph.adjacency
graph.data.order <- melt(Mrow, id.vars = c('From', 'To'))
graph.data.order <- melt(b, id.vars = c('From', 'To'))
View(graph.data.order)
colnames(graph.data.order) <- c('source','target','weight')
View(graph.data.order)
m=as.matrix(graph.data.order)
net=graph.adjacency(adjmatrix=m,mode="directed",weighted=TRUE,diag=FALSE)
View(m)
View(b)
E(simple.email.graph)$weight
count(E(simple.email.graph)$Cod)
E(simple.email.graph)$Cod
h <- head(people,500)
library(dplyr)
library(reshape2)
library(igraph)
data <- read.csv(file="data/top_users.csv",head=TRUE,sep=",")
#STEP 1: CREATE THE ADJACENCY MATRIX FROM FIRST TWO COLUMNS
people <- select(data, From, To)
M = as.matrix(table(h)) # restructure your network data in matrix format
M = as.matrix(table(people)) # restructure your network data in matrix format
m = table(people)
M = as.matrix(m)
Mrow = M %*% t(M)
View(Mrow)
graph.data.order <- melt(b, id.vars = c('From', 'To'))
colnames(graph.data.order) <- c('source','target','weight')
graph.data.order <- melt(b, id.vars = c('From', 'To'))
library(dplyr)
library(reshape2)
library(igraph)
data <- read.csv(file="data/top_users.csv",head=TRUE,sep=",")
h <- head(data, 500) #just to test
h$weight <-1
#STEP 1: CREATE THE ADJACENCY MATRIX FROM FIRST TWO COLUMNS
people <- select(data, From, To, Cod)
email.graph <- graph.data.frame(people, directed=TRUE)
simple.email.graph <- simplify(email.graph, remove.loops = T, remove.multiple = F)
b <- get.adjacency(simple.email.graph,sparse=FALSE) #to see the matrix
graph.data.order <- melt(b, id.vars = c('From', 'To'))
colnames(graph.data.order) <- c('source','target','weight')
View(graph.data.order)
b[,1]=as.character(el[,1])
b[,1]=as.character(b[,1])
graph.data.order[,1]=as.character(graph.data.order[,1])
m <- as.matrix(graph.data.order)
View(m)
colnames(graph.data.order) <- c('From','To','weight')
graph.data.order <- melt(b, id.vars = c('From', 'To'))
colnames(graph.data.order) <- c('From','To','weight')
View(graph.data.order)
graph.data.order <- melt(b, id.vars = c('From'))
colnames(graph.data.order) <- c('From','To','weight')
uniendo <- inner_join(people,graph.data.order)
View(uniendo)
uno <- select(uniendo, From=="neil.davies@enron.com" )
uno <- select(uniendo, From == "neil.davies@enron.com" )
s <- filter(uniendo, From %in% c("neil.davies@enron.com")) #just a test for one
View(s)
View(people)
graph.data.order <- melt(b, id.vars = c('From'))
colnames(graph.data.order) <- c('source','target','weight')
g2=graph.data.frame(graph.data.order)
list.edge.attributes(g2)
get.data.frame(g2, what="edges")
s <- filter(data, From %in% c("sladana-anna.kulic@enron.com")) #just a test for one
View(s)
graph.data.order <- melt(b, id.vars = c('From'))
colnames(graph.data.order) <- c('From','To','weight')
uniendo <- left_join(people,graph.data.order)
View(uniendo)
s <- filter(data, To %in% c("mary.cook@enron.com")) #just a test for one
View(s)
dos <- select(graph.data.order, From == "neil.davies@enron.com" )
s <- filter(graph.data.order, To %in% c("mary.cook@enron.com")) #just a test for one
s <- filter(data, To %in% c("mary.cook@enron.com")) #just a test for one
mary <- filter(graph.data.order, To %in% c("mary.cook@enron.com")) #just a test for one
View(s)
View(mary)
s <- filter(uniendo, To %in% c("mary.cook@enron.com")) #just a test for one
mary <- filter(graph.data.order, To %in% c("mary.cook@enron.com")) #just a test for one
View(mary)
View(s)
s$weight <-1
View(s)
View(s)
g3<-graph.data.frame(s)
?simplify()
s.a <- get.adjacency(g3,sparse=FALSE)
View(s.a)
list.edge.attributes(s.a)
list.edge.attributes(g3)
E(g3)$weight
simplify(g3, edge.attr.comb="sum")
simplify(g3, edge.attr.comb=list(weight="sum", "ignore")
simplify(g3, edge.attr.comb=list(weight="sum", "ignore"))
simplify(g3, edge.attr.comb=list(weight="sum", "ignore"))
E(g3)$weight
simplify(g3, edge.attr.comb=list(weight="sum", "ignore"))
V(g3)$weight
g4<- implify(g3, edge.attr.comb=list(weight="sum", "ignore"))
g4<- simplify(g3, , remove.loops = T, remove.multiple = F, edge.attr.comb=list(weight="sum", "ignore"))
g4<- simplify(g3, remove.loops = T, remove.multiple = F, edge.attr.comb=list(weight="sum", "ignore"))
s.b <- get.adjacency(g4,sparse=FALSE)
View(s.b)
V(g4)$weight
E(g4)$weight
graph.data.order[,1]=as.character(graph.data.order[,1])
el=as.matrix(graph.data.order)
g=graph.edgelist(el[,1:2])
E(g)$weight=as.numeric(el[,3])
E(g)$weight
list.edge.attributes(g) #this only has the attribute weight
View(uniendo)
source("iGraph_GexF_Exporter.R")
saveAsGEXF(g, "enron_graph.gexf")
graph.authors.edge <- subgraph.edges(g, which(E(g)$weight >= 1))
graph.senders <- subgraph.edges(g, which(E(g)$weight >= 1))
graph.enron<- subgraph.edges(g, which(E(g)$weight >= 1))
source("iGraph_GexF_Exporter.R")
saveAsGEXF(graph.enron, "enron_graph.gexf")
install.packages("rgexf")
install.packages("XML")
new.people <- people
colnames(new.people) <- c('source','target', 'id')
write.table(b, file="data/graph_matrix.csv", row.names=FALSE, sep=",")
write.table(b, file="data/graph_with_id.csv", row.names=FALSE, sep=",")
write.table(new.people, file="data/graph_with_id.csv", row.names=FALSE, sep=",")
View(new.people)
people$id<-seq.int(nrow(people))
View(people)
colnames(new.people) <- c('source','target', 'label', 'id')
colnames(people) <- c('source','target', 'label', 'id')
View(people)
write.table(people, file="data/graph_with_id.csv", row.names=FALSE, sep=",")
View(b)
data <- read.csv(file="data/emails.csv",head=TRUE,sep=",")
data <- read.csv(file="data/emails.csv",head=TRUE,sep=",")
View(data)
data <- read.csv(file="data/emails.csv",head=TRUE,sep=",")
View(data)
View(data)
install.packages("tm")
docs <- c("This is a text.", "This another one.")
(vs <- VectorSource(docs))
library(tm)
(vs <- VectorSource(docs))
inspect(VCorpus(vs))
View(data)
docs <- head(data,10)
vs <- VectorSource(docs)
inspect(VCorpus(vs))
docs <- head(data,10)
vs <- VectorSource(docs$email)
inspect(VCorpus(vs))
inspect(VCorpus(emails))
emails <- VectorSource(docs$email) #this loads the documents from the data frame into a vector courpus, just for emails
inspect(VCorpus(emails))
myCorpus <- VectorSource(docs$email) #this loads the documents from the data frame into a vector courpus, just for emails
inspect(VCorpus(myCorpus))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
corpus=tm_map(myCcorpus,tolower)
corpus=tm_map(myCorpus,tolower)
?tm_map
docs <- head(data,1)
myCorpus <- VectorSource(docs$email) #this loads the documents from the data frame into a vector courpus, just for emails
inspect(VCorpus(myCorpus))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
GreekShippingContent <- "The Greek administration is coming under increasing pressure over it foot-dragging regarding its meeting international convention deadlines, especially when it relies on classification societies as an Recognised Organisation (RO) on its behalf. "
GreekShippingContent0 <-  Corpus(VectorSource(GreekShippingContent))
tm_map(GreekShippingContent0, PlainTextDocument)
?VectorSource
myCorpus <- VectorSource(docs$email) #this loads the documents from the data frame into a vector courpus, just for emails
tm_map(myCorpus0,PlainTextDocument)
tm_map(myCorpus,PlainTextDocument)
myCorpus
docs <- c("RDataMining: CLAVIN: an open source software package for document geotagging and geoparsing http://t.co/gTGbTanKCI")
myCorpus <- VectorSource(docs) #this loads the documents from the data frame into a vector courpus, just for emails
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- VectorSource(docs) #this loads the documents from the data frame into a vector courpus, just for emails
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
docs <- head(data,1)
myCorpus <- VectorSource(docs$Subject) #this loads the documents from the data frame into a vector courpus, just for emails
inspect(VCorpus(myCorpus))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
View(docs)
?Corpus
corpus=Corpus(VectorSource(docs$Subject))
corpus<-Corpus(VectorSource(docs$Subject))
inspect(VCorpus(emails))
myCorpus <- tm_map(emails, content_transformer(tolower))
myCorpus <- tm_map(emails, tolower)
View(docs)
docs <- head(data,1)
emails<-Corpus(VectorSource(docs$Subject))
docs <- head(data,10)
emails<-Corpus(VectorSource(docs$Subject))
View(docs)
inspect(VCorpus(emails))
inspect(VCorpus(emails))
emails<-Corpus(VectorSource(docs$Subject))
inspect(VCorpus(emails))
?VectorSource
x<-c("Hello. Sir!","Tacos? On Tuesday?!?")
mycorpus <- Corpus(VectorSource(x))
mycorpus <- tm_map(mycorpus, removePunctuation)
docs$Subject
a <- as.data.frame(docs$Subject)
View(a)
colnames(a, "subject")
a
View(data)
docs <-select(Cod, email)
library(dplyr)
docs <-select(Cod, email)
docs <-select(docs, Cod, email)
View(docs)
emails<-Corpus(VectorSource(docs$email))
a <- as.data.frame(docs$Subject)
emails
inspect(VCorpus(emails))
mycorpus <- tm_map(emails, removePunctuation)
View(docs)
myCorpus <- tm_map(emails, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
inspect(myCorpus)
for (i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
#writeLines(myCorpus[[i]])
writeLines(as.character(myCorpus[[i]]))
}
?stopwords
stopwords("en")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
for (i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
#writeLines(myCorpus[[i]])
writeLines(as.character(myCorpus[[i]]))
}
install.packages("SnowballC")
library(SnowballC)
myCorpus <- tm_map(myCorpus, stemDocument)
#To inspect the corpus anytime I want
for (i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
#writeLines(myCorpus[[i]])
writeLines(as.character(myCorpus[[i]]))
}
myStopwords <- c('enron', 'subject', 'www', 'com', 'http', 'https', 'cc', 'message', 'sent', 're', 'ect', 'etc', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'll', 'am', 'pm')
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
for (i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
#writeLines(myCorpus[[i]])
writeLines(as.character(myCorpus[[i]]))
}
myCorpus <- tm_map(myCorpus, stripWhitespace)
#To inspect the corpus anytime I want
for (i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
#writeLines(myCorpus[[i]])
writeLines(as.character(myCorpus[[i]]))
}
docs <- tm_map(myCorpus, PlainTextDocument)
DTM <- DocumentTermMatrix(docs)
DTM
sparse_DTM <- removeSparseTerms(DTM, 0.97)
sparse_DTM
inspect(sparse_DTM)
inspect(DTM)
sparse_DTM <- removeSparseTerms(DTM, 0.1)
inspect(sparse_DTM)
sparse_DTM <- removeSparseTerms(DTM, 0.1)
inspect(sparse_DTM)
sparse_DTM <- removeSparseTerms(DTM, 0.9)
inspect(sparse_DTM)
sparse_DTM <- removeSparseTerms(DTM, 0.97)
inspect(sparse_DTM)
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.90)
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.80)
sparse_DTM
?removeSparseTerms
sparse_DTM <- removeSparseTerms(DTM, 0.2)
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.5)
sparse_DTM
inspect(sparse_DTM)
DTM <- DocumentTermMatrix(docs) #This builds the document terms matrix
DTM
sparse_DTM <- removeSparseTerms(DTM, 0.90)
inspect(sparse_DTM)
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.85)
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.87)
sparse_DTM
inspect(sparse_DTM)
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.88) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM #75% sparsity
sparse_DTM <- removeSparseTerms(DTM, 0.90) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM #75% sparsity
DTM
docs <- head(data,100)
docs <-select(docs, Cod, email) #I had to filter it again because there is some trash in the data
emails<-Corpus(VectorSource(docs$email))
myCorpus <- tm_map(emails, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myStopwords <- c('enron', 'subject', 'www', 'com', 'http', 'https', 'cc', 'message', 'sent', 're', 'ect', 'etc', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'll', 'am', 'pm')
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stripWhitespace)
docs <- tm_map(myCorpus, PlainTextDocument)
DTM <- DocumentTermMatrix(docs) #This builds the document terms matrix
DTM
sparse_DTM <- removeSparseTerms(DTM, 0.95) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM #75% sparsity
sparse_DTM <- removeSparseTerms(DTM, 0.97) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM #75% sparsity
(freq.terms <- findFreqTerms(DTM, lowfreq=15))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- rowSums(as.matrix(DTM))
term.freq <- subset(term.freq, term.freq >=5)
df <- data.frame(term = names(term.freq), freq = term.freq)
View(df)
freq <- colSums(as.matrix(DTM))
length(freq)
freq[head(ord)]
ord <- order(freq)
freq[head(ord)]
freq[tail(ord)]
head(table(freq), 20)
freq
head(freq,10)
freq <- sort(colSums(as.matrix(DTM)), decreasing=TRUE)
head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq)
View(wf)
install.packages("ggplot2")
library(ggplot2)
#Plotting the term frequency using ggplot
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
docs <-select(data, Cod, email) #I had to filter it again because there is some trash in the data
emails<-Corpus(VectorSource(docs$email))
library(dplyr)
library(tm)
library(SnowballC)
library(ggplot2)
data <- read.csv(file="data/emails.csv",head=TRUE,sep=",")
docs <-select(data, Cod, email) #I had to filter it again because there is some trash in the data
emails<-Corpus(VectorSource(docs$email))
myCorpus <- tm_map(emails, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myStopwords <- c('enron', 'subject', 'will','www', 'com', 'http', 'https', 'cc', 'message', 'sent', 're', 'ect', 'etc', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'll', 'am', 'pm')
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stripWhitespace)
docs <- tm_map(myCorpus, PlainTextDocument)
DTM <- DocumentTermMatrix(docs) #This builds the document terms matrix
sparse_DTM <- removeSparseTerms(DTM, 0.97) #if I set it to 0.90 then nothing changes, under that it does
write.csv(DTM, file="dtm.csv")
DTM
sparse_DTM <- removeSparseTerms(DTM, 0.97) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.97) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM
freq <- sort(colSums(as.matrix(DTM)), decreasing=TRUE)
freq <- sort(colSums(as.matrix(sparse_DTM)), decreasing=TRUE)
head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq) #A data frame with the frequency of each term
tail(freq, 14)
p <- ggplot(subset(wf, freq>20000), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
write(wf, file = "terms_frequency_inbox.csv")
write.table(wf, file="data/terms_frequency_emails.csv", row.names=FALSE, sep=",")
write.table(DTM, file="data/DTM.csv", row.names=FALSE, sep=",")
sparse_DTM
sparse_DTM <- removeSparseTerms(DTM, 0.99) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM #93% sparsity
freq <- sort(colSums(as.matrix(sparse_DTM)), decreasing=TRUE)
head(freq, 14)
install.packages("sentiment")
library(dplyr)
library(tm)
library(SnowballC)
library(ggplot2)
docs <- head(docs,500)
library(dplyr)
library(tm)
library(SnowballC)
library(ggplot2)
data <- read.csv(file="data/emails.csv",head=TRUE,sep=",")
emails<-Corpus(VectorSource(docs$processed_text))
library(dplyr)
library(tm)
library(SnowballC)
library(ggplot2)
data <- read.csv(file="data/clean_emails.csv", head=TRUE, sep=",")
docs <-select(data, processed_text) #I had to filter it again because there is some trash in the data
emails<-Corpus(VectorSource(docs$processed_text))
DTM <- DocumentTermMatrix(emails) #This builds the document terms matrix
DTM
sparse_DTM <- removeSparseTerms(DTM, 0.99) #if I set it to 0.90 then nothing changes, under that it does
inspect(sparse_DTM)
sparse_DTM #93% sparsity
sparse_DTM <- removeSparseTerms(DTM, 0.97) #if I set it to 0.90 then nothing changes, under that it does
sparse_DTM #93% sparsity
DTM
freq <- sort(colSums(as.matrix(sparse_DTM)), decreasing=TRUE)
head(freq, 14)
tail(freq, 14)
wf <- data.frame(word=names(freq), freq=freq) #A data frame with the frequency of each term
p <- ggplot(subset(wf, freq>20000), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>19000), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>18000), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
View(data)
library(dplyr)
clean <- read.csv(file="data/clean_emails.csv", head=TRUE, sep=",")
graph <- read.csv(file="data/graph_with_id.csv", head=TRUE, sep=",")
colnames(graph) <- c('source', 'target', 'Cod', 'id')
unique_mails <- distinct(select(graph, label)) #10 thousand emails...uhmm a lot
colnames(unique_mails) <- c('Cod')
unique_mails <- distinct(select(graph, label)) #10 thousand emails...uhmm a lot
colnames(graph) <- c('source', 'target', 'Cod', 'id')
unique_mails <- distinct(select(graph, label)) #10 thousand emails...uhmm a lot
View(graph)
colnames(graph) <- c('source', 'target', 'label', 'id')
unique_mails <- distinct(select(graph, label)) #10 thousand emails...uhmm a lot
colnames(unique_mails) <- c('Cod')
#This are the emails that I will really need to preprocess and get the sentiment from
emails.to.process <- filter(clean, Cod %in% unique_mails$Cod)
